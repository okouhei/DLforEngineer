{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深層学習 day1-day2 実装\n",
    "\n",
    "## 概要\n",
    "ここでは、day1に関わる項目を復習しながら、全結合層を実装していきます。\n",
    "ゼロかつくるディープラーニングを参考にしながら、2つの実装を行なっていきます。\n",
    "主にday1-day2に跨る内容(CNNや最適化以外)を復習していきます。(day2はもう1つ実装演習をしています)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 事前準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# 乱数シードを指定\n",
    "np.random.seed(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの読み込み・線形結合層などの定義\n",
    "\n",
    "3_DNN_day1で実装した関数やクラスを流用します。\n",
    "同じくMNISTを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnistデータセットのロード(ネットワーク接続が必要・少し時間がかかります)\n",
    "if os.path.exists('mnist_784'):\n",
    "    with open('mnist_784','rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "else:\n",
    "    mnist = datasets.fetch_openml('mnist_784')\n",
    "    with open('mnist_784', 'wb') as f:\n",
    "        pickle.dump(mnist, f)\n",
    "    \n",
    "# 画像とラベルを取得\n",
    "X, T = mnist.data, mnist.target\n",
    "# 訓練データとテストデータに分割\n",
    "X_train, X_test, T_train, T_test = train_test_split(X, T, test_size=0.2)\n",
    "# ラベルデータをint型にし、one-hot-vectorに変換します\n",
    "T_train = np.eye(10)[T_train.astype(\"int\")]\n",
    "T_test = np.eye(10)[T_test.astype(\"int\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer():\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        self.w = np.random.randn(input_shape, output_shape) * 0.01\n",
    "        self.b = np.zeros(output_shape, dtype=np.float)\n",
    "        self.x = None\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(self.x, self.w) + self.b\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.w.T)\n",
    "        batch_size = dx.shape[0]\n",
    "        self.dw = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCrossEntropyLoss():\n",
    "    def __init__(self):\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        self.loss = None\n",
    "        \n",
    "    def __call__(self, t, y):\n",
    "        self.y = softmax(y)\n",
    "        self.t = t.copy()\n",
    "        self.loss = cross_entropy_error(self.t, self.y)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dy = self.y - self.t\n",
    "        dy /= batch_size\n",
    "        return dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x = x.T\n",
    "    _x = x - np.max(x, axis=0)\n",
    "    _x = np.exp(_x) / np.sum(np.exp(_x), axis=0)\n",
    "    return _x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(t, y):\n",
    "    delta = 1e-8\n",
    "    error = -np.mean(t * np.log(y + delta))\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ドロップアウト\n",
    "\n",
    "1. <font color=\"Red\">ドロップアウトクラスを実装します。</font>\n",
    "    - 入力されてきた ```x``` に対し、確率 ```dropout_ratio``` で出力を0にする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout():\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "\n",
    "    def __call__(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            randommatrix = np.random.rand(*x.shape)\n",
    "            self.mask = randommatrix > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "    \n",
    "#参考 https://qiita.com/ishikawa-takumi/items/598dcd185a49b02883d5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### パラメータノルムペナルティ\n",
    "\n",
    "2-1. <font color=\"Red\">ベクトル・行列に対してLpノルムを計算する関数```lp_norm```を実装します。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lp_norm(x, p=2):\n",
    "    return np.sum(np.abs(x**p))**(1/p) \n",
    "#参考　https://qiita.com/hkthirano/items/9750290cb34a3e4570f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実装したドロップアウトクラスと、パラメータノルムペナルティを使って、ドロップアウトとL2ノルム正則化（重み減衰）を含めたNNを実装してみます。\n",
    "\n",
    "[重要!!!]\n",
    "- 損失関数に $ \\frac{\\lambda}{2}\\|w\\|_2^2 $を正則化項として加える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_classifier():\n",
    "\n",
    "    def __init__(self, weight_decay_lambda=0):\n",
    "        '''\n",
    "        構造\n",
    "        x -> fc(783, 256) -> relu -> dropout -> fc(256, 256) -> relu -> dropout -> fc(256, 10) -> out\n",
    "        '''\n",
    "        \n",
    "        # 層の定義\n",
    "        self.fc1 = FullyConnectedLayer(784, 256)\n",
    "        self.relu1 = ReLU()\n",
    "        self.dropout1 = Dropout()\n",
    "        self.fc2 = FullyConnectedLayer(256, 256)\n",
    "        self.relu2 = ReLU()\n",
    "        self.dropout2 = Dropout()\n",
    "        self.fc3 = FullyConnectedLayer(256, 10)\n",
    "        self.out = None\n",
    "        \n",
    "        # 損失関数の定義\n",
    "        self.criterion = SoftmaxCrossEntropyLoss()\n",
    "        self.weight_decay_lambda = weight_decay_lambda\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        '''\n",
    "        順伝播\n",
    "        '''\n",
    "        \n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.dropout1(x, train_flg)\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.dropout2(x, train_flg)\n",
    "        self.out = self.fc3(x)\n",
    "        \n",
    "        # 勾配計算の都合上softmaxはこの順伝播関数内では行わない\n",
    "        # 予測するときはさらにsoftmaxを通す必要がある\n",
    "        return self.out\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        loss = self.criterion(t, self.forward(x))\n",
    "        for fc in [self.fc1, self.fc2, self.fc3]:\n",
    "            loss += (self.weight_decay_lambda/2) * lp_norm(fc.w, p=2)**2 ##重要　ここで正則化項を罰則として与えている##\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        '''\n",
    "        逆伝播\n",
    "        '''\n",
    "        # 勾配を逆伝播\n",
    "        d = self.criterion.backward()\n",
    "        d = self.fc3.backward(d)\n",
    "        d = self.dropout2.backward(d)\n",
    "        d = self.relu2.backward(d)\n",
    "        d = self.fc2.backward(d)\n",
    "        d = self.dropout1.backward(d)\n",
    "        d = self.relu1.backward(d)\n",
    "        d = self.fc1.backward(d)\n",
    "        \n",
    "        for fc in [self.fc1, self.fc2, self.fc3]:\n",
    "            fc.dw += self.weight_decay_lambda * fc.w ##重要##\n",
    "\n",
    "    def optimize_GradientDecent(self, lr):\n",
    "        '''\n",
    "        勾配降下法による全層のパラメータの更新\n",
    "        '''\n",
    "        for fc in [self.fc1, self.fc2, self.fc3]:\n",
    "            fc.w -= lr * fc.dw\n",
    "            fc.b -= lr * fc.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習\n",
    "train_loss が1.4前後、accuracyが80%前後の結果になりました。より精度を高める場合には、パラメータ最適化法などが考えられると思いますが、今回の実装演習では精度が確保できました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 | TRAIN LOSS 1.57379 | ACCURACY 19.41%\n",
      "EPOCH 2 | TRAIN LOSS 1.56571 | ACCURACY 29.18%\n",
      "EPOCH 3 | TRAIN LOSS 1.55825 | ACCURACY 37.47%\n",
      "EPOCH 4 | TRAIN LOSS 1.55084 | ACCURACY 43.16%\n",
      "EPOCH 5 | TRAIN LOSS 1.54349 | ACCURACY 47.99%\n",
      "EPOCH 6 | TRAIN LOSS 1.53618 | ACCURACY 51.86%\n",
      "EPOCH 7 | TRAIN LOSS 1.52864 | ACCURACY 55.35%\n",
      "EPOCH 8 | TRAIN LOSS 1.52106 | ACCURACY 58.36%\n",
      "EPOCH 9 | TRAIN LOSS 1.51311 | ACCURACY 61.25%\n",
      "EPOCH 10 | TRAIN LOSS 1.50461 | ACCURACY 63.52%\n",
      "EPOCH 11 | TRAIN LOSS 1.49584 | ACCURACY 65.43%\n",
      "EPOCH 12 | TRAIN LOSS 1.48692 | ACCURACY 67.10%\n",
      "EPOCH 13 | TRAIN LOSS 1.47694 | ACCURACY 68.45%\n",
      "EPOCH 14 | TRAIN LOSS 1.46680 | ACCURACY 69.31%\n",
      "EPOCH 15 | TRAIN LOSS 1.45661 | ACCURACY 70.41%\n",
      "EPOCH 16 | TRAIN LOSS 1.44611 | ACCURACY 71.56%\n",
      "EPOCH 17 | TRAIN LOSS 1.43593 | ACCURACY 72.55%\n",
      "EPOCH 18 | TRAIN LOSS 1.42626 | ACCURACY 73.66%\n",
      "EPOCH 19 | TRAIN LOSS 1.41661 | ACCURACY 74.61%\n",
      "EPOCH 20 | TRAIN LOSS 1.40783 | ACCURACY 75.64%\n",
      "EPOCH 21 | TRAIN LOSS 1.39989 | ACCURACY 76.33%\n",
      "EPOCH 22 | TRAIN LOSS 1.39198 | ACCURACY 77.02%\n",
      "EPOCH 23 | TRAIN LOSS 1.38516 | ACCURACY 77.57%\n",
      "EPOCH 24 | TRAIN LOSS 1.37826 | ACCURACY 77.98%\n",
      "EPOCH 25 | TRAIN LOSS 1.37195 | ACCURACY 78.54%\n",
      "EPOCH 26 | TRAIN LOSS 1.36543 | ACCURACY 78.84%\n",
      "EPOCH 27 | TRAIN LOSS 1.35990 | ACCURACY 79.47%\n",
      "EPOCH 28 | TRAIN LOSS 1.35391 | ACCURACY 79.69%\n",
      "EPOCH 29 | TRAIN LOSS 1.34853 | ACCURACY 80.30%\n",
      "EPOCH 30 | TRAIN LOSS 1.34327 | ACCURACY 80.12%\n"
     ]
    }
   ],
   "source": [
    "# モデルの宣言\n",
    "model = MLP_classifier(weight_decay_lambda=0.1)\n",
    "\n",
    "# 学習率\n",
    "lr = 0.02\n",
    "# 学習エポック数\n",
    "n_epoch = 30\n",
    "\n",
    "# n_epoch繰り返す\n",
    "for n in range(n_epoch):\n",
    "    loss = model.loss(X_train, T_train)    \n",
    "    model.backward()\n",
    "    \n",
    "    model.optimize_GradientDecent(lr)\n",
    "        \n",
    "    # テスト\n",
    "    y = model.forward(X_test, train_flg=False)\n",
    "    pred = softmax(y)\n",
    "    accuracy = np.mean(np.equal(np.argmax(y, axis=1), np.argmax(T_test, axis=1)))\n",
    "    \n",
    "    print(f'EPOCH {n + 1} | TRAIN LOSS {loss:.5f} | ACCURACY {accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
